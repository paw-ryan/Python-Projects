{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"Copy of 03-09-Cross Validation.ipynb","provenance":[{"file_id":"1GdzjgLoGaXI5cfmT0bmohWfdyM9QXZyW","timestamp":1572424038720},{"file_id":"1pD0ouRAj6XHNmyvf-WmoyH1HCFx3dIRX","timestamp":1570953471908}],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"a5Unl4zKHhH5","colab_type":"text"},"source":["# Cross Validation (CV)"]},{"cell_type":"markdown","metadata":{"id":"kH1bPhLeHhH7","colab_type":"text"},"source":["* Hold out Cross Validation\n","* k-fold Cross Validation"]},{"cell_type":"markdown","metadata":{"id":"pgtXQaWqHhH9","colab_type":"text"},"source":["A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. \n","\n","In the basic approach, called k-fold CV, the training set is split into k smaller sets. The following procedure is followed for each of the k “folds”:\n","* A model is trained using k-1 of the folds as training data;\n","* the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n","\n","The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. "]},{"cell_type":"markdown","metadata":{"id":"JlHH_3PsHhH-","colab_type":"text"},"source":["# Holdout Method"]},{"cell_type":"markdown","metadata":{"id":"p39JUcDNHhH-","colab_type":"text"},"source":["* Split initial dataset into a separate training and test dataset\n","* Training dataset - model training\n","* Test dataset - estimate its generalisation performance"]},{"cell_type":"markdown","metadata":{"id":"Z0MD_vs3HhH_","colab_type":"text"},"source":["<img src='img//cv_holdout_method.png' width=50%>"]},{"cell_type":"markdown","metadata":{"id":"QD1UVbvzHhIA","colab_type":"text"},"source":["A variation is to split the training set to two :- training set and validation set\n","\n","Training set:- For fitting different models\n","\n","Validation set :- For tuning and comparing different parameter settings to further improve the performance for making predictions on unseen data. And finally for model selection.\n","\n","This process is called model selection. We want to select the optimal values of tuning parameters (also called hyperparameters). \n"]},{"cell_type":"markdown","metadata":{"id":"WyIn_KgAHhIB","colab_type":"text"},"source":["<img src='img//cv_holdout_w_validation.png' width=50%>"]},{"cell_type":"markdown","metadata":{"id":"weHx9bDHHhIC","colab_type":"text"},"source":["# K-fold Cross-validation"]},{"cell_type":"markdown","metadata":{"id":"_p3u0xrRHhID","colab_type":"text"},"source":["* Randomly split the training dataset into k folds without replacement.\n","\n","* k — 1 folds are used for the model training.\n","\n","* The one fold is used for performance evaluation. \n","\n","This procedure is repeated k times. \n","\n","Final outcomes:- k models and performance estimates.\n","\n","* calculate the average performance of the models based on the different, independent folds to obtain a performance estimate that is less sensitive to the sub-partitioning of the training data compared to the holdout method. \n","\n","* k-fold cross-validation is used for model tuning. Finding the optimal hyperparameter values that yields a satisfying generalization performance.\n","\n","* Once we have found satisfactory hyperparameter values, we can retrain the model on the complete training set and obtain a final performance estimate using the independent test set. The rationale behind fitting a model to the whole training dataset after k-fold cross-validation is that providing more training samples to a learning algorithm usually results in a more accurate and robust model.\n","\n","\n","* Common k is 10\n","\n","* For relatively small training sets, increase the number of folds. "]},{"cell_type":"markdown","metadata":{"id":"JYG7EOFZHhIF","colab_type":"text"},"source":["<img src='img//cv_cross_validation.png' width=50%>"]},{"cell_type":"markdown","metadata":{"id":"nCUYrMn3HhIG","colab_type":"text"},"source":["## Stratified k-fold cross-validation\n","\n","* variation of k-fold\n","* Can yield better bias and variance estimates, especially in cases of unequal class proportions"]},{"cell_type":"markdown","metadata":{"id":"r91SEDdTHhIH","colab_type":"text"},"source":["***"]},{"cell_type":"markdown","metadata":{"id":"iIgXsNAAHhIH","colab_type":"text"},"source":["# Illustration"]},{"cell_type":"markdown","metadata":{"id":"Q6HhiQ1DHhII","colab_type":"text"},"source":["## Cross-validation: evaluating estimator performance\n","\n","Adapted from [scikit learn](Cross-validation: evaluating estimator performance)"]},{"cell_type":"markdown","metadata":{"id":"tQdC9oh7HhIJ","colab_type":"text"},"source":["Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called **overfitting**. \n","\n","To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a **test set** `X_test`, `y_test`. Note that the word “experiment” is not intended to denote academic use only, because even in commercial settings machine learning usually starts out experimentally."]},{"cell_type":"markdown","metadata":{"id":"hye17G01HhIJ","colab_type":"text"},"source":["In scikit-learn a random split into training and test sets can be quickly computed with the `train_test_split` helper function. Let’s load the iris data set to fit a linear support vector machine on it:"]},{"cell_type":"code","metadata":{"id":"C56dSOmiHhIL","colab_type":"code","outputId":"80414137-38f4-442e-b3f1-ffd1578dfe24","executionInfo":{"status":"ok","timestamp":1570957808483,"user_tz":-480,"elapsed":1664,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","sns.set_style(\"whitegrid\")\n","%matplotlib inline\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn import datasets\n","from sklearn import svm\n","\n","boston = datasets.load_boston()\n","boston.data.shape, boston.target.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((506, 13), (506,))"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"GK535c1nHhIO","colab_type":"code","outputId":"37e2ed3e-dd8f-4c8d-93cd-c1f81dd42aaf","executionInfo":{"status":"ok","timestamp":1570957808486,"user_tz":-480,"elapsed":1642,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["print(np.__version__)\n","print(pd.__version__)\n","import sys\n","print(sys.version)\n","import sklearn\n","print(sklearn.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.16.5\n","0.24.2\n","3.6.8 (default, Jan 14 2019, 11:02:34) \n","[GCC 8.0.1 20180414 (experimental) [trunk revision 259383]]\n","0.21.3\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Byk7TPwtHhIR","colab_type":"text"},"source":["We can now quickly sample a training set while holding out 40% of the data for testing (evaluating) our regressor:"]},{"cell_type":"code","metadata":{"id":"yLWhJUbgHhIS","colab_type":"code","outputId":"a4d7e744-80e6-4441-b97d-c5a299002510","executionInfo":{"status":"ok","timestamp":1570957810285,"user_tz":-480,"elapsed":3427,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.4, random_state=0)\n","\n","print(X_train.shape, y_train.shape)\n","\n","print(X_test.shape, y_test.shape)\n","\n","regression = svm.SVR(kernel='linear', C=1).fit(X_train, y_train)\n","regression.score(X_test, y_test)      "],"execution_count":0,"outputs":[{"output_type":"stream","text":["(303, 13) (303,)\n","(203, 13) (203,)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.667431382173115"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"9CDrNYoZHhIU","colab_type":"text"},"source":["When evaluating different settings (“hyperparameters”) for estimators, such as the `C` setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. \n","\n","This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. \n","\n","To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n","\n","However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"baIVvKCsHhIV","colab_type":"text"},"source":["A solution to this problem, as discussed earlier, is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n","\n","* A model is trained using k-1 of the folds as training data;\n","* the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n","\n","The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as it is the case when fixing an arbitrary test set), which is a major advantage in problem such as inverse inference where the number of samples is very small."]},{"cell_type":"markdown","metadata":{"id":"xHp_w_8rHhIW","colab_type":"text"},"source":["## Computing cross-validated metrics"]},{"cell_type":"code","metadata":{"id":"kko0SFhJHhIX","colab_type":"code","outputId":"d17ce381-c004-46d0-d9e1-872390c97ce2","executionInfo":{"status":"ok","timestamp":1570957826669,"user_tz":-480,"elapsed":19798,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.model_selection import cross_val_score\n","regression = svm.SVR(kernel='linear', C=1)\n","scores = cross_val_score(regression, boston.data, boston.target, cv=5)\n","scores        "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.77285459, 0.72771739, 0.56131914, 0.15056451, 0.08212844])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"LLa9HJUdHhIZ","colab_type":"text"},"source":["The mean score and the 95% confidence interval of the score estimate are hence given by:"]},{"cell_type":"code","metadata":{"id":"o-khLXPmHhIa","colab_type":"code","outputId":"f3150191-e6b1-4516-b4bf-05e075bc527e","executionInfo":{"status":"ok","timestamp":1570957826670,"user_tz":-480,"elapsed":19785,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std()))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy: 0.46 (+/- 0.29)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"h8TtjxSPHhId","colab_type":"text"},"source":["By default, the score computed at each CV iteration is the score method of the estimator. It is possible to change this by using the scoring parameter:"]},{"cell_type":"code","metadata":{"id":"1u7dTz4eHhIe","colab_type":"code","outputId":"6c00a612-7cba-4b72-8255-738cdfbe971e","executionInfo":{"status":"ok","timestamp":1570957842862,"user_tz":-480,"elapsed":35958,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn import metrics\n","scores = cross_val_score(regression, boston.data, boston.target, cv=5, scoring='neg_mean_squared_error')\n","scores "],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ -7.84451123, -24.78772444, -35.13272326, -74.50555945,\n","       -24.40465975])"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"OH85XBM7HhIg","colab_type":"text"},"source":["See [The scoring parameter: defining model evaluation rules](http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) for details. In the case of the Iris dataset, the samples are balanced across target classes hence the accuracy and the F1-score are almost equal."]},{"cell_type":"markdown","metadata":{"id":"-FtDRW5SHhIh","colab_type":"text"},"source":["When the `cv` argument is an integer, `cross_val_score` uses the `KFold` or `StratifiedKFold` strategies by default, the latter being used if the estimator derives from ClassifierMixin."]},{"cell_type":"markdown","metadata":{"id":"r5KzcgwwHhIi","colab_type":"text"},"source":["## K-fold"]},{"cell_type":"markdown","metadata":{"id":"S0nmcRu1HhIj","colab_type":"text"},"source":["`KFold` divides all the samples in k groups of samples, called folds (if k = n, this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using k - 1 folds, and the fold left out is used for test.\n","\n","Example of 2-fold cross-validation on a dataset with 4 samples:"]},{"cell_type":"code","metadata":{"id":"wADCeXz1HhIj","colab_type":"code","outputId":"7cc5bc37-e910-41e6-91f9-be1284bfab84","executionInfo":{"status":"ok","timestamp":1570957842864,"user_tz":-480,"elapsed":35946,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import numpy as np\n","from sklearn.model_selection import KFold\n","\n","X = [\"a\", \"b\", \"c\", \"d\"]\n","kf = KFold(n_splits=2)\n","for train, test in kf.split(X):\n","    print(\"%s %s\" % (train, test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[2 3] [0 1]\n","[0 1] [2 3]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"WcScAxKqHhIm","colab_type":"text"},"source":["## Stratified k-fold"]},{"cell_type":"markdown","metadata":{"id":"nnYWlVVTHhIn","colab_type":"text"},"source":["StratifiedKFold is a variation of k-fold which returns stratified folds: each set contains approximately the same percentage of samples of each target class as the complete set.\n","\n","Example of stratified 3-fold cross-validation on a dataset with 10 samples from two slightly unbalanced classes:"]},{"cell_type":"code","metadata":{"id":"Wf9X13c6HhIo","colab_type":"code","outputId":"b7129af3-463a-47ee-8bd5-83aeabdb499d","executionInfo":{"status":"ok","timestamp":1570957842865,"user_tz":-480,"elapsed":35932,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from sklearn.model_selection import StratifiedKFold\n","\n","X = np.ones(10)\n","y = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n","skf = StratifiedKFold(n_splits=3)\n","for train, test in skf.split(X, y):\n","    print(\"%s %s\" % (train, test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[2 3 6 7 8 9] [0 1 4 5]\n","[0 1 3 4 5 8 9] [2 6 7]\n","[0 1 2 4 5 6 7] [3 8 9]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IB1saWaaHhIq","colab_type":"code","outputId":"11bc75bc-bbb5-4080-9039-393bdd676b1c","executionInfo":{"status":"ok","timestamp":1570957842866,"user_tz":-480,"elapsed":35916,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"ktSyba91HhI5","colab_type":"code","outputId":"4afd7549-b135-42bc-9781-e59721577874","executionInfo":{"status":"ok","timestamp":1570957842866,"user_tz":-480,"elapsed":35903,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0, 0, 0, 0, 1, 1, 1, 1, 1, 1]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"CbFYRqyEHhI8","colab_type":"code","outputId":"04d45d86-3b91-4864-cf8c-ffd9ab4b1c6d","executionInfo":{"status":"ok","timestamp":1570957842867,"user_tz":-480,"elapsed":35893,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","#from sklearn.linear_model import LogisticRegression\n","from sklearn import svm\n","from sklearn.pipeline import make_pipeline\n","#pipe_lr = make_pipeline(StandardScaler(),\n","#                        PCA(n_components=2),\n","#                        LogisticRegression(random_state=1))\n","pipe_svm = make_pipeline(StandardScaler(),\n","                         PCA(n_components=2),\n","                         svm.SVR(kernel='linear', C=1))\n","pipe_svm.fit(X_train, y_train)\n","y_pred = pipe_svm.predict(X_test)\n","print('Test Accuracy: %.3f' % pipe_svm.score(X_test, y_test))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Test Accuracy: 0.391\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mw-UEsoLHhI_","colab_type":"code","outputId":"8904e273-2887-476c-e24d-1b5e0cd4ba65","executionInfo":{"status":"ok","timestamp":1570957842868,"user_tz":-480,"elapsed":35882,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["from sklearn.model_selection import cross_val_score\n","scores = cross_val_score(estimator=pipe_svm,\n","                         X=X_train,\n","                         y=y_train,\n","                         cv=10,\n","                         n_jobs=1)\n","print('CV accuracy scores: %s' % scores)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CV accuracy scores: [0.63971176 0.43579197 0.46977821 0.25027246 0.5124364  0.26221374\n"," 0.30877195 0.54528563 0.37810066 0.47313549]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0mTMN-VJHhJC","colab_type":"code","outputId":"7ef5613a-e7b1-4275-d3a8-8063eecd3053","executionInfo":{"status":"ok","timestamp":1570957842869,"user_tz":-480,"elapsed":35871,"user":{"displayName":"Anthony Ng","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mDCwHDw3XUA0T7h5JCQA3iBUS8Wbz1V-29O8pmP8g=s64","userId":"06915612711739148186"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores),\n","                                      np.std(scores)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["CV accuracy: 0.428 +/- 0.121\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QaUiNQ57HhJJ","colab_type":"text"},"source":["***"]}]}